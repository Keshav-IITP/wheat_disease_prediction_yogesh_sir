{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2038b3-470a-4051-bf73-2b2064a85dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8afcb07-e69a-4c86-8b7e-950d9b18e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1, CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'   \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9707737a-160c-480d-af83-a872f94ff453",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   \n",
    "    transforms.RandomHorizontalFlip(p=0.2), \n",
    "    transforms.RandomRotation(10),   \n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.1)),   \n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    " \n",
    "train_dataset = datasets.ImageFolder(\"C:/Users/SOHAM/python/kaggle_WCD/data/train\", transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(\"C:/Users/SOHAM/python/kaggle_WCD/data/valid\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08db81e-cdbd-4ebd-952c-7f619474c145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747fbed8-34f4-40c9-9b87-040e09cb8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatCNN(nn.Module):\n",
    "    def __init__(self, num_classes=15):  \n",
    "        super(WheatCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2c638f-071b-44f5-a5a0-86f6baeff63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on cuda\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/30\n",
      "Batch 0/410 Loss: 2.732\n",
      "Batch 10/410 Loss: 2.653\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/30\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m     61\u001b[0m     val_loss, val_acc, val_f1, preds, labels \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion, device)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m running_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m     13\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torchvision\\datasets\\folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\PIL\\Image.py:1007\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mor\u001b[39;00m (mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matrix):\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix:\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# matrix conversion\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\PIL\\Image.py:1274\u001b[0m, in \u001b[0;36mImage.copy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;124;03mCopies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;124;03minto an image, but still retain the original.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m-> 1274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mcopy())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = WheatCNN(num_classes=15).to(device)\n",
    "print(f\"Model on {device}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    " \n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(loader)} Loss: {loss.item():.3f}')\n",
    "    scheduler.step()\n",
    "    return running_loss/len(loader), correct/total\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    val_acc = correct / total\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return running_loss/len(loader), val_acc, val_f1, all_preds, all_labels\n",
    "\n",
    " \n",
    "epochs = 30\n",
    "best_val_acc = 0\n",
    "best_val_f1 = 0\n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "\n",
    " \n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/30\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_f1, preds, labels = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train: {train_acc:.1%} (Loss: {train_loss:.3f})\")\n",
    "    print(f\"  Val:   {val_acc:.1%} (F1: {val_f1:.3f})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if val_acc > best_val_acc or val_f1 > best_val_f1:\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        best_val_f1 = max(best_val_f1, val_f1)\n",
    "        torch.save(model.state_dict(), 'best_wheat_cnn_aggressive.pth')\n",
    "        patience_counter = 0\n",
    "        print(f\" NEW BEST: {best_val_acc:.1%} (F1: {best_val_f1:.3f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n FINAL REPORT\")\n",
    "print(classification_report(labels, preds, target_names=train_dataset.classes))\n",
    "print(f\"BEST: best_wheat_cnn_aggressiv.pth | {best_val_acc:.1%} F1 {best_val_f1:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9b9a88e-1e42-4722-8fa5-fa9fb46c2585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: 13104 | Val: 600\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(\"C:/Users/SOHAM/python/kaggle_WCD/data/train\", transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(\"C:/Users/SOHAM/python/kaggle_WCD/data/valid\", transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Train: {len(train_dataset)} | Val: {len(val_dataset)}\")  \n",
    "\n",
    " \n",
    "checkpoint = torch.load(r\"C:\\Users\\SOHAM\\best_wheat_cnn_aggressive.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(\"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8030d0-3e57-4654-91fb-5a95536d4856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/30\n",
      "Batch 0/410 Loss: 1.274\n",
      "Batch 10/410 Loss: 1.402\n",
      "Batch 20/410 Loss: 1.784\n",
      "Batch 30/410 Loss: 1.740\n",
      "Batch 40/410 Loss: 1.731\n",
      "Batch 50/410 Loss: 1.468\n",
      "Batch 60/410 Loss: 1.852\n",
      "Batch 70/410 Loss: 1.370\n",
      "Batch 80/410 Loss: 1.468\n",
      "Batch 90/410 Loss: 1.403\n",
      "Batch 100/410 Loss: 1.661\n",
      "Batch 110/410 Loss: 1.077\n",
      "Batch 120/410 Loss: 1.495\n",
      "Batch 130/410 Loss: 1.226\n",
      "Batch 140/410 Loss: 1.313\n",
      "Batch 150/410 Loss: 1.528\n",
      "Batch 160/410 Loss: 1.388\n",
      "Batch 170/410 Loss: 1.806\n",
      "Batch 180/410 Loss: 1.214\n",
      "Batch 190/410 Loss: 1.624\n",
      "Batch 200/410 Loss: 1.398\n",
      "Batch 210/410 Loss: 1.708\n",
      "Batch 220/410 Loss: 1.724\n",
      "Batch 230/410 Loss: 1.341\n",
      "Batch 240/410 Loss: 1.418\n",
      "Batch 250/410 Loss: 1.284\n",
      "Batch 260/410 Loss: 1.475\n",
      "Batch 270/410 Loss: 1.456\n",
      "Batch 280/410 Loss: 1.329\n",
      "Batch 290/410 Loss: 1.100\n",
      "Batch 300/410 Loss: 1.931\n",
      "Batch 310/410 Loss: 1.513\n",
      "Batch 320/410 Loss: 1.728\n",
      "Batch 330/410 Loss: 1.410\n",
      "Batch 340/410 Loss: 1.177\n",
      "Batch 350/410 Loss: 1.243\n",
      "Batch 360/410 Loss: 1.774\n",
      "Batch 370/410 Loss: 1.304\n",
      "Batch 380/410 Loss: 1.531\n",
      "Batch 390/410 Loss: 1.988\n",
      "Batch 400/410 Loss: 1.572\n",
      "  Train: 52.0% (Loss: 1.511)\n",
      "  Val:   38.6% (F1: 0.335)\n",
      " NEW BEST: 38.6%\n",
      "\n",
      "Epoch 12/30\n",
      "Batch 0/410 Loss: 1.538\n",
      "Batch 10/410 Loss: 1.655\n",
      "Batch 20/410 Loss: 1.267\n",
      "Batch 30/410 Loss: 1.346\n",
      "Batch 40/410 Loss: 1.685\n",
      "Batch 50/410 Loss: 1.482\n",
      "Batch 60/410 Loss: 1.517\n",
      "Batch 70/410 Loss: 1.541\n",
      "Batch 80/410 Loss: 1.279\n",
      "Batch 90/410 Loss: 1.603\n",
      "Batch 100/410 Loss: 1.112\n",
      "Batch 110/410 Loss: 1.272\n",
      "Batch 120/410 Loss: 1.358\n",
      "Batch 130/410 Loss: 1.059\n",
      "Batch 140/410 Loss: 2.128\n",
      "Batch 150/410 Loss: 1.345\n",
      "Batch 160/410 Loss: 1.442\n",
      "Batch 170/410 Loss: 1.419\n",
      "Batch 180/410 Loss: 1.577\n",
      "Batch 190/410 Loss: 1.292\n",
      "Batch 200/410 Loss: 1.224\n",
      "Batch 210/410 Loss: 1.463\n",
      "Batch 220/410 Loss: 1.254\n",
      "Batch 230/410 Loss: 1.557\n",
      "Batch 240/410 Loss: 1.614\n",
      "Batch 250/410 Loss: 1.357\n",
      "Batch 260/410 Loss: 1.291\n",
      "Batch 270/410 Loss: 1.253\n",
      "Batch 280/410 Loss: 1.332\n",
      "Batch 290/410 Loss: 1.584\n",
      "Batch 300/410 Loss: 1.068\n",
      "Batch 310/410 Loss: 1.545\n",
      "Batch 320/410 Loss: 1.616\n",
      "Batch 330/410 Loss: 1.643\n",
      "Batch 340/410 Loss: 1.622\n",
      "Batch 350/410 Loss: 1.940\n",
      "Batch 360/410 Loss: 1.577\n",
      "Batch 370/410 Loss: 1.671\n",
      "Batch 380/410 Loss: 1.579\n",
      "Batch 390/410 Loss: 1.481\n",
      "Batch 400/410 Loss: 1.296\n",
      "  Train: 53.3% (Loss: 1.473)\n",
      "  Val:   43.7% (F1: 0.383)\n",
      " NEW BEST: 43.7%\n",
      "\n",
      "Epoch 13/30\n",
      "Batch 0/410 Loss: 1.726\n",
      "Batch 10/410 Loss: 1.505\n",
      "Batch 20/410 Loss: 1.489\n",
      "Batch 30/410 Loss: 1.185\n",
      "Batch 40/410 Loss: 1.521\n",
      "Batch 50/410 Loss: 1.229\n",
      "Batch 60/410 Loss: 1.679\n",
      "Batch 70/410 Loss: 1.148\n",
      "Batch 80/410 Loss: 1.446\n",
      "Batch 90/410 Loss: 1.409\n",
      "Batch 100/410 Loss: 1.316\n",
      "Batch 110/410 Loss: 1.425\n",
      "Batch 120/410 Loss: 1.467\n",
      "Batch 130/410 Loss: 1.715\n",
      "Batch 140/410 Loss: 1.513\n",
      "Batch 150/410 Loss: 1.212\n",
      "Batch 160/410 Loss: 1.365\n",
      "Batch 170/410 Loss: 1.657\n",
      "Batch 180/410 Loss: 1.294\n",
      "Batch 190/410 Loss: 1.092\n",
      "Batch 200/410 Loss: 1.767\n",
      "Batch 210/410 Loss: 1.226\n",
      "Batch 220/410 Loss: 1.459\n",
      "Batch 230/410 Loss: 1.278\n",
      "Batch 240/410 Loss: 1.318\n",
      "Batch 250/410 Loss: 1.702\n",
      "Batch 260/410 Loss: 1.236\n",
      "Batch 270/410 Loss: 1.247\n",
      "Batch 280/410 Loss: 1.957\n",
      "Batch 290/410 Loss: 1.772\n",
      "Batch 300/410 Loss: 1.052\n",
      "Batch 310/410 Loss: 1.776\n",
      "Batch 320/410 Loss: 1.204\n",
      "Batch 330/410 Loss: 1.231\n",
      "Batch 340/410 Loss: 1.428\n",
      "Batch 350/410 Loss: 1.522\n",
      "Batch 360/410 Loss: 1.361\n",
      "Batch 370/410 Loss: 1.482\n",
      "Batch 380/410 Loss: 1.259\n",
      "Batch 390/410 Loss: 1.385\n",
      "Batch 400/410 Loss: 1.412\n",
      "  Train: 54.0% (Loss: 1.452)\n",
      "  Val:   43.5% (F1: 0.381)\n",
      "\n",
      "Epoch 14/30\n",
      "Batch 0/410 Loss: 1.550\n",
      "Batch 10/410 Loss: 1.144\n",
      "Batch 20/410 Loss: 1.164\n",
      "Batch 30/410 Loss: 1.822\n",
      "Batch 40/410 Loss: 1.696\n",
      "Batch 50/410 Loss: 1.228\n",
      "Batch 60/410 Loss: 1.135\n",
      "Batch 70/410 Loss: 1.374\n",
      "Batch 80/410 Loss: 1.151\n",
      "Batch 90/410 Loss: 1.488\n",
      "Batch 100/410 Loss: 1.473\n",
      "Batch 110/410 Loss: 1.745\n",
      "Batch 120/410 Loss: 1.676\n",
      "Batch 130/410 Loss: 0.989\n",
      "Batch 140/410 Loss: 1.463\n",
      "Batch 150/410 Loss: 1.088\n",
      "Batch 160/410 Loss: 1.407\n",
      "Batch 170/410 Loss: 1.725\n",
      "Batch 180/410 Loss: 1.747\n",
      "Batch 190/410 Loss: 1.665\n",
      "Batch 200/410 Loss: 1.402\n",
      "Batch 210/410 Loss: 1.301\n",
      "Batch 220/410 Loss: 1.209\n",
      "Batch 230/410 Loss: 1.380\n",
      "Batch 240/410 Loss: 1.235\n",
      "Batch 250/410 Loss: 1.431\n",
      "Batch 260/410 Loss: 1.077\n",
      "Batch 270/410 Loss: 1.628\n",
      "Batch 280/410 Loss: 1.273\n",
      "Batch 290/410 Loss: 1.434\n",
      "Batch 300/410 Loss: 1.197\n",
      "Batch 310/410 Loss: 1.668\n",
      "Batch 320/410 Loss: 1.696\n",
      "Batch 330/410 Loss: 1.134\n",
      "Batch 340/410 Loss: 1.577\n",
      "Batch 350/410 Loss: 1.565\n",
      "Batch 360/410 Loss: 1.896\n",
      "Batch 370/410 Loss: 1.085\n",
      "Batch 380/410 Loss: 1.795\n",
      "Batch 390/410 Loss: 1.356\n",
      "Batch 400/410 Loss: 1.434\n",
      "  Train: 54.5% (Loss: 1.428)\n",
      "  Val:   45.8% (F1: 0.406)\n",
      " NEW BEST: 45.8%\n",
      "\n",
      "Epoch 15/30\n",
      "Batch 0/410 Loss: 1.446\n",
      "Batch 10/410 Loss: 1.562\n",
      "Batch 20/410 Loss: 1.412\n",
      "Batch 30/410 Loss: 1.632\n",
      "Batch 40/410 Loss: 1.260\n",
      "Batch 50/410 Loss: 1.419\n",
      "Batch 60/410 Loss: 1.391\n",
      "Batch 70/410 Loss: 1.168\n",
      "Batch 80/410 Loss: 1.510\n",
      "Batch 90/410 Loss: 1.404\n",
      "Batch 100/410 Loss: 1.530\n",
      "Batch 110/410 Loss: 1.134\n",
      "Batch 120/410 Loss: 1.504\n",
      "Batch 130/410 Loss: 1.221\n",
      "Batch 140/410 Loss: 0.915\n",
      "Batch 150/410 Loss: 1.803\n",
      "Batch 160/410 Loss: 1.105\n",
      "Batch 170/410 Loss: 1.447\n",
      "Batch 180/410 Loss: 1.551\n",
      "Batch 190/410 Loss: 1.398\n",
      "Batch 200/410 Loss: 1.448\n",
      "Batch 210/410 Loss: 1.234\n",
      "Batch 220/410 Loss: 1.250\n",
      "Batch 230/410 Loss: 1.371\n",
      "Batch 240/410 Loss: 1.624\n",
      "Batch 250/410 Loss: 1.698\n",
      "Batch 260/410 Loss: 1.395\n",
      "Batch 270/410 Loss: 0.873\n",
      "Batch 280/410 Loss: 1.658\n",
      "Batch 290/410 Loss: 1.215\n",
      "Batch 300/410 Loss: 1.413\n",
      "Batch 310/410 Loss: 1.483\n",
      "Batch 320/410 Loss: 1.414\n",
      "Batch 330/410 Loss: 1.433\n",
      "Batch 340/410 Loss: 1.342\n",
      "Batch 350/410 Loss: 1.269\n",
      "Batch 360/410 Loss: 1.081\n",
      "Batch 370/410 Loss: 1.640\n",
      "Batch 380/410 Loss: 0.872\n",
      "Batch 390/410 Loss: 1.346\n",
      "Batch 400/410 Loss: 1.371\n",
      "  Train: 55.4% (Loss: 1.416)\n",
      "  Val:   44.7% (F1: 0.407)\n",
      "\n",
      "Epoch 16/30\n",
      "Batch 0/410 Loss: 1.256\n",
      "Batch 10/410 Loss: 1.295\n",
      "Batch 20/410 Loss: 1.725\n",
      "Batch 30/410 Loss: 1.246\n",
      "Batch 40/410 Loss: 1.207\n",
      "Batch 50/410 Loss: 1.183\n",
      "Batch 60/410 Loss: 1.557\n",
      "Batch 70/410 Loss: 1.568\n",
      "Batch 80/410 Loss: 1.227\n",
      "Batch 90/410 Loss: 1.605\n",
      "Batch 100/410 Loss: 1.271\n",
      "Batch 110/410 Loss: 1.110\n",
      "Batch 120/410 Loss: 1.255\n",
      "Batch 130/410 Loss: 1.506\n",
      "Batch 140/410 Loss: 1.666\n",
      "Batch 150/410 Loss: 1.619\n",
      "Batch 160/410 Loss: 1.715\n",
      "Batch 170/410 Loss: 1.710\n",
      "Batch 180/410 Loss: 1.710\n",
      "Batch 190/410 Loss: 1.515\n",
      "Batch 200/410 Loss: 1.490\n",
      "Batch 210/410 Loss: 1.213\n",
      "Batch 220/410 Loss: 1.409\n",
      "Batch 230/410 Loss: 1.524\n",
      "Batch 240/410 Loss: 1.130\n",
      "Batch 250/410 Loss: 1.469\n",
      "Batch 260/410 Loss: 1.127\n",
      "Batch 270/410 Loss: 1.593\n",
      "Batch 280/410 Loss: 1.361\n",
      "Batch 290/410 Loss: 1.061\n",
      "Batch 300/410 Loss: 1.602\n",
      "Batch 310/410 Loss: 1.403\n",
      "Batch 320/410 Loss: 1.104\n",
      "Batch 330/410 Loss: 1.345\n",
      "Batch 340/410 Loss: 1.682\n",
      "Batch 350/410 Loss: 1.295\n",
      "Batch 360/410 Loss: 1.330\n",
      "Batch 370/410 Loss: 1.939\n",
      "Batch 380/410 Loss: 1.391\n",
      "Batch 390/410 Loss: 1.244\n",
      "Batch 400/410 Loss: 1.760\n",
      "  Train: 55.3% (Loss: 1.409)\n",
      "  Val:   46.4% (F1: 0.417)\n",
      " NEW BEST: 46.4%\n",
      "\n",
      "Epoch 17/30\n",
      "Batch 0/410 Loss: 1.432\n",
      "Batch 10/410 Loss: 1.476\n",
      "Batch 20/410 Loss: 1.131\n",
      "Batch 30/410 Loss: 1.388\n",
      "Batch 40/410 Loss: 1.074\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m):   \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/30\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m      9\u001b[0m     val_loss, val_acc, val_f1, preds, labels \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion, device)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m running_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m     13\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torchvision\\datasets\\folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\PIL\\Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_wheat_cnn_aggressive.pth', map_location=device, weights_only=False))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)  # \n",
    " \n",
    "\n",
    " \n",
    "for epoch in range(10, 30):   \n",
    "    print(f\"\\nEpoch {epoch+1}/30\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_f1, preds, labels = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train: {train_acc:.1%} (Loss: {train_loss:.3f})\")\n",
    "    print(f\"  Val:   {val_acc:.1%} (F1: {val_f1:.3f})\")\n",
    "    \n",
    "     \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_wheat_cnn_aggressive.pth')\n",
    "        print(f\" NEW BEST: {best_val_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecca1276-2631-4893-8ac8-fb9954cce7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Classes: ['Aphid', 'Black Rust', 'Blast', 'Brown Rust', 'Common Root Rot', 'Fusarium Head Blight', 'Healthy', 'Leaf Blight', 'Mildew', 'Mite', 'Septoria', 'Smut', 'Stem fly', 'Tan spot', 'Yellow Rust'] num_classes: 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    " \n",
    "train_dir = r\"C:/Users/SOHAM/python/kaggle_WCD/data/train\"\n",
    "val_dir   = r\"C:/Users/SOHAM/python/kaggle_WCD/data/valid\"\n",
    " \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    " \n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset   = datasets.ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(\"Classes:\", train_dataset.classes, \"num_classes:\", num_classes)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58996aae-5e82-4c46-a6d7-04dc44025530",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ResNet18_Weights.IMAGENET1K_V1  # pretrained ImageNet\n",
    "model = models.resnet18(weights=weights)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "ckpt_path = \"best_resnet18_wheat.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0848ba8-0737-41e3-9ab6-4261f04d746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Head] Epoch 1/5 Train Loss: 1.846 Acc: 0.503 Val Loss: 1.783 Acc: 0.559\n",
      " New BEST (head): 0.5587786259541985\n",
      "[Head] Epoch 2/5 Train Loss: 1.549 Acc: 0.614 Val Loss: 1.707 Acc: 0.579\n",
      " New BEST (head): 0.5786259541984733\n",
      "[Head] Epoch 3/5 Train Loss: 1.496 Acc: 0.632 Val Loss: 1.697 Acc: 0.623\n",
      " New BEST (head): 0.6229007633587786\n",
      "[Head] Epoch 4/5 Train Loss: 1.480 Acc: 0.645 Val Loss: 1.662 Acc: 0.600\n",
      "[Head] Epoch 5/5 Train Loss: 1.455 Acc: 0.658 Val Loss: 1.684 Acc: 0.609\n",
      "[FT] Epoch 1/20 Train Loss: 1.267 Acc: 0.736 Val Loss: 1.248 Acc: 0.777\n",
      "New BEST (fine‑tune): 0.7770992366412214\n",
      "[FT] Epoch 2/20 Train Loss: 1.045 Acc: 0.824 Val Loss: 1.063 Acc: 0.844\n",
      "New BEST (fine‑tune): 0.8442748091603054\n",
      "[FT] Epoch 3/20 Train Loss: 0.945 Acc: 0.866 Val Loss: 1.017 Acc: 0.844\n",
      "[FT] Epoch 4/20 Train Loss: 0.880 Acc: 0.890 Val Loss: 1.007 Acc: 0.849\n",
      "New BEST (fine‑tune): 0.8488549618320611\n",
      "[FT] Epoch 5/20 Train Loss: 0.835 Acc: 0.908 Val Loss: 0.945 Acc: 0.902\n",
      "New BEST (fine‑tune): 0.9022900763358779\n",
      "[FT] Epoch 6/20 Train Loss: 0.798 Acc: 0.926 Val Loss: 0.900 Acc: 0.898\n",
      "[FT] Epoch 7/20 Train Loss: 0.775 Acc: 0.934 Val Loss: 0.893 Acc: 0.916\n",
      "New BEST (fine‑tune): 0.916030534351145\n",
      "[FT] Epoch 8/20 Train Loss: 0.750 Acc: 0.941 Val Loss: 0.880 Acc: 0.910\n",
      "[FT] Epoch 9/20 Train Loss: 0.731 Acc: 0.947 Val Loss: 0.875 Acc: 0.901\n",
      "[FT] Epoch 10/20 Train Loss: 0.711 Acc: 0.954 Val Loss: 0.850 Acc: 0.915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m num_epochs_ft \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs_ft):\n\u001b[1;32m---> 71\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m     72\u001b[0m     val_loss, val_acc     \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_loader, criterion, device)\n\u001b[0;32m     73\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m running_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      5\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\condaaa\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = out.argmax(1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    " \n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith(\"fc.\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                       lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "num_epochs_head = 5\n",
    "\n",
    "for epoch in range(num_epochs_head):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc     = eval_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"[Head] Epoch {epoch+1}/{num_epochs_head} \"\n",
    "          f\"Train Loss: {train_loss:.3f} Acc: {train_acc:.3f} \"\n",
    "          f\"Val Loss: {val_loss:.3f} Acc: {val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "        }, ckpt_path)\n",
    "        print(\" New BEST (head):\", best_val_acc)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "num_epochs_ft = 20\n",
    "for epoch in range(num_epochs_ft):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc     = eval_epoch(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"[FT] Epoch {epoch+1}/{num_epochs_ft} \"\n",
    "          f\"Train Loss: {train_loss:.3f} Acc: {train_acc:.3f} \"\n",
    "          f\"Val Loss: {val_loss:.3f} Acc: {val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            \"epoch\": num_epochs_head + epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "        }, ckpt_path)\n",
    "        print(\"New BEST (fine‑tune):\", best_val_acc)\n",
    "\n",
    "print(\"Training done. Best val acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "058adb53-07e7-4916-9e6a-f70a1378305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classes: ['aphid_test', 'black_rust_test', 'blast_test', 'brown_rust_test', 'common_root_rot_test', 'fusarium_head_blight_test', 'healthy_test', 'leaf_blight_test', 'mildew_test', 'mite_test', 'septoria_test', 'smut_test', 'stem_fly_test', 'tan_spot_test', 'yellow_rust_test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOHAM\\AppData\\Local\\Temp\\ipykernel_8028\\3122050831.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"best_resnet18_wheat.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best val acc: 0.916030534351145\n",
      "Test accuracy: 0.8800  (660/750)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "test_dir = r\"C:/Users/SOHAM/python/kaggle_WCD/data/test\"\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"Test classes:\", test_dataset.classes)\n",
    "\n",
    "# 2) Rebuild ResNet18 and load best weights\n",
    "model = models.resnet18(weights=None)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, 15)   # 15 classes\n",
    "model.to(device)\n",
    "\n",
    "ckpt = torch.load(\"best_resnet18_wheat.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(\"Loaded best val acc:\", ckpt[\"best_val_acc\"])\n",
    "\n",
    "# 3) Evaluate on test\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test accuracy: {test_acc:.4f}  ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b57cc381-4b00-458e-a9c4-0d92c0fc43ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not wheat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    " \n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "classes = test_dataset.classes \n",
    " \n",
    "def predict_image(img_path, model, transform, classes, device):\n",
    "    model.eval()\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(image)          # C,H,W\n",
    "    x = x.unsqueeze(0).to(device) # 1,C,H,W\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred_idx = probs.argmax(1).item()\n",
    "        pred_prob = probs[0, pred_idx].item()\n",
    "\n",
    "    return classes[pred_idx], pred_prob\n",
    "\n",
    "# ----- 3) USE IT -----\n",
    "img_path = r\"C:\\Users\\SOHAM\\OneDrive\\画像\\Camera Roll\\WIN_20251116_09_23_29_Pro.jpg\" # change to your image path\n",
    "label, conf = predict_image(img_path, model, img_transform, classes, device)\n",
    " \n",
    "label = label.replace(\"_test\", \"\").replace(\"_\", \" \").title()\n",
    "if conf * 100 > 75:\n",
    "    print(f\"Predicted: {label}  (confidence: {conf:.3f})\")\n",
    "else:\n",
    "    print(\"Not wheat\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb0ecd7-5af2-4624-95f0-624f04e1413d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aphid_test',\n",
       " 'black_rust_test',\n",
       " 'blast_test',\n",
       " 'brown_rust_test',\n",
       " 'common_root_rot_test',\n",
       " 'fusarium_head_blight_test',\n",
       " 'healthy_test',\n",
       " 'leaf_blight_test',\n",
       " 'mildew_test',\n",
       " 'mite_test',\n",
       " 'septoria_test',\n",
       " 'smut_test',\n",
       " 'stem_fly_test',\n",
       " 'tan_spot_test',\n",
       " 'yellow_rust_test']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2855d95-048c-43c9-9814-7645387e66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[49  0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0 46  0  3  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0 50  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 44  0  0  0  0  2  0  3  0  0  0  1]\n",
      " [ 0  0  0  0 47  0  0  0  0  2  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 49  0  0  0  0  0  1  0  0  0]\n",
      " [ 1  0  2  0  0  0  4  0  0  0  0  0  0  0 43]\n",
      " [ 2  1  0  0  0  0  0 42  1  0  1  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0 49  1  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  4  0 40  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0 48  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 50  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 50  0  0]\n",
      " [ 1  0  0  0  1  0  0  4  0  1  1  0  0 42  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:       \n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds  = np.array(all_preds)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)   \n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "626e4e9f-1fa6-417c-9cdc-9f023e970f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/drive/folders/1Ymm1ILnmFEChThGNbQTd3KTe-eeZwtZn?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "618060cd-7ab6-421a-926d-8ce8a8a938e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What ive done is, tried implementing a new CNN model but due to overfitting, I used a pre trained model. If the accuracy to predict is less than 80%, \n",
    "# output as no wheat in frame as the pre trained model is accurate when it comes to wheat diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ace7ef5-dd7f-4bd1-aea2-62ea83c2298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OpenCV pipeline implementing the above idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f575f20-0861-4a86-85ba-127553233b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
